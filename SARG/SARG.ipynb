{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b><h1> Stacked Approximation Regression Machines from First Principles </b></h1> (Gabriel Goh)</center>\n",
    "\n",
    "<img src = \"bases.png\">\n",
    "\n",
    "There has been some buzz on [reddit](https://www.reddit.com/r/MachineLearning/comments/50tbjp/stacked_approximated_regression_machine_a_simple/) about the paper [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/abs/1608.04062). Approaching the paper with a measured dose of skepticism, I was pleasantly surprised to find the paper containing a beautiful kernel of an idea - one I can see becoming a fixture in our deep learning toolkit, the $k$-Approximated Regression Machine ($k$-ARM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background - Deep Nets\n",
    "\n",
    "A deep neural network is a function which takes as input, $x$, data, into an output, $F(x)$. The word “deep” in deep learning refers to the network being a composition of layer functions:\n",
    "$$F(x)=\\Psi^{m}(\\,\\Psi^{m-1}(\\,\\cdots\\,\\Psi^{1}(x)\\,\\cdots\\,)\\,)$$\n",
    "\n",
    "A traditional choice of a layer function looks like $\\Psi^{k}(x)=\\sigma(W^{T}_k x)$. Here $W^{T}_k$ is a matrix, representing a linear transform, (such as a convolution or a fully connected layer) and $\\sigma$ is a choice of a non-linear [activation function](https://en.wikipedia.org/wiki/Activation_function). The goal in deep learning is to shape our function $F$, by any means possible, by tweaking the weights till they fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Regression Machine\n",
    "\n",
    "While traditional tools worked well, there has been recently a Cambrian explosion of nontraditional layers. Consider the following conceptual layer. This layer isn't defined explicitly, but is instead defined implicitly as the solution of a *sparse coding problem*:\n",
    "$$\\Psi(x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+\\lambda\\|y\\|_{1}\\big\\}.$$\n",
    "\n",
    "I dub this the regression machine. \n",
    "\n",
    "This layer behaves very differently from a regular deep learning layer and it might be worthwhile to consider what it does. $W$ is a set of weights, to be trained, and $\\lambda$ is a parameter controlling how sparse the output is. Assuming a trained model, with sensible $W$, this layer is a sparse encoder. It takes $x$, it's input and transforms it into its representation in structural primitives (the columns of $W$). \n",
    "\n",
    "It is useful to think of a musical metaphor. The sound of a chord (our input $x$) is generated by a sparse superposition of sounds corresponding to a small number of notes (our sparse parameter $y$). Our layer takes this generative process and throws it in reverse, and attempts to, from the sound of a chord, to recover the individual notes. \n",
    "\n",
    "If you have a background in engineering, these terms might be familiar to you in a different language. The generative process can be seen as the forward model, \n",
    "\n",
    "<img src=\"sparse.svg\" width = 450px>\n",
    "\n",
    "and the problem of recovering the notes from its output, the inverse problem\n",
    "\n",
    "<img src=\"inverse.svg\" width = 550px>\n",
    "\n",
    "This layer is therefore, paradoxically, the opposite of a regular deep learning layer. But it is this lopsided layer which makes the most sense, as it truly takes data from input space into explanation space - a higher level of abstraction. It's no surprise then that this is the mechanism proposed by [Olshausen et al](http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf) to represent the early stages of visual processing.\n",
    "\n",
    "Despite its implicit definition, we can still take its (sub) gradient,\n",
    "$$\\begin{align}\\nabla\\Psi(x) & =W^{T}(Wy^\\star-x)+\\lambda\\mbox{sign}(y^\\star)\\\\\n",
    "\\frac{\\partial}{\\partial W_{ij}}\\Psi(x) & =\\frac{\\partial}{\\partial W_{ij}}\\frac{1}{2}\\|Wy^\\star-x\\|^{2}\n",
    "\\end{align}$$\n",
    "\n",
    "by plugging $y^\\star = \\Phi(x)$, the solution to the sparse coding problem. Hence there is no technical roadblock in integrating this into a deep learning framework. \n",
    "\n",
    "This leads to the troubling question - why isn't this kind of layer de-facto? The trouble with this layer is that it is expensive. Computing this layer requires the solution of a small optimization problem (on the order of the number of features in a datum). Measured in milliseconds, this may be cheap -  the cost of solving this problem is on the order of solving a linear system. But it is still an order of magnitude more expensive than a simple matrix multiplication passed through a nonlinearity. Adding a single layer like this would potentially increase the cost of every training step a hundredfold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $k$-ARM\n",
    "\n",
    "Let us instead try to approximate this layer. First, I define a (seemingly) cryptic operator\n",
    "$$\n",
    "\\Psi_{x}'(y) := \\sigma(y-\\alpha W^{T}(Wy-x)), \\qquad \\sigma(y)_{i}=\\mbox{sign}(y_i)\\max\\{0,\\left| y_{i} \\right|-\\lambda\\}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is a number strictly greater than zero, but smaller than the largest eigenvector of $W^{T}W$. Let's think about what this operator does. Let $f(y)=\\tfrac{1}{2}\\|Wy-x\\|^{2}$, our quadratic term in the objective. The term inside the $\\sigma$ is then just $y - \\alpha\\nabla f(y)$, a gradient step. The nonlinearity, $\\sigma$, snaps all components of this new iterate between $[-\\lambda, \\lambda]$ to zero. So our operator consists of two steps. The first step improves the objective, and the second promotes sparsity. Furthermore, let's take these two facts as given:\n",
    "\n",
    "1. $\\Psi_x$ is the unique fixed point of the map $y\\mapsto \\Psi_{x}'(y),$ i.e. $\\Psi(x)=\\Psi_{x}'(\\Psi(x))$\n",
    "2. From any inital point $y$, repeated application of the map will always converge to $\\Psi(x).$\n",
    "\n",
    "In informal but suggestive notation, \n",
    "$$\\Psi(x)=\\Psi_{x}'(\\Psi_{x}'(\\Psi_{x}'(\\cdots))).$$\n",
    "\n",
    "Which suggests a series of approximations $\\Psi\\approx\\Psi_{k}$, (starting at $0$ for simplicity),\n",
    "$$\\begin{align}\n",
    "\\Psi_{0}(x) & =\\Psi_{x}'(\\mathbf{0})\\\\\n",
    "\\Psi_{1}(x) & =\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0}))\\\\\n",
    "\\Psi_{2}(x) & =\\Psi'_{x}(\\Psi_{x}'(\\Psi_{x}'(\\mathbf{0})))\\\\\n",
    " & \\vdots\\\\\n",
    "\\lim_{k\\rightarrow\\infty}\\Psi_{k}(x) & =\\Psi(x).\n",
    "\\end{align}.$$\n",
    "\n",
    "This approximation has an architectural diagram looking like this\n",
    "\n",
    "<p>\n",
    "<img src = \"diagram.svg\" width = 500px>\n",
    "<p>\n",
    "\n",
    "And our most aggressive approximation, $\\Psi_0(x)$ has the form\n",
    "$$\n",
    "\\Psi_{0}(x)\t\n",
    "  =\\Psi_{x}'(\\mathbf{0})\n",
    "  =\\sigma(\\mathbf{0}-\\tfrac{1}{\\alpha}W^{T}(W\\mathbf{0}-x))\n",
    "  =\\sigma(\\tfrac{1}{\\alpha}W^{T}x)\n",
    "$$\n",
    "\n",
    "which should look familiar. Why, it's nothing more than our traditional layer discussed at the beginning! A tantalizing coincidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generalized $k$-ARM\n",
    "\n",
    "##### Proximal Gradient\n",
    "Where does the operator $\\Psi_{x}'(y)$ come from? There is, in fact, a powerful framework in convex optimization which gives us the tools to craft such operators. The study of proximal operators gives us a means to solve problems of the form\n",
    "$$y^\\star = \\mbox{argmin}\\{f(y)+g(y)\\}$$\n",
    "\n",
    "Where $f$ is smooth and convex and $g$ is just convex. For reasons which will be clear later, $f$ is usually chosen to be the vessel of our data, and $g$ to be structurally simple, like the 1-norm. Sparse coding, discussed earlier, is a special case of this where\n",
    "$$f(y)=\\tfrac{1}{2}\\|Wy-x\\|^{2},\\qquad g(y)=\\lambda\\|y\\|_{1}$$\n",
    "\n",
    "This framework gives us a recipie on how to replace the 1-norm with any convex function $g$, such as any p-norm, grouped 2-norms and more.\n",
    "##### The Proximal Gradient Iteration\n",
    "\n",
    "Given $f$ and $g$, the proximal gradient method defines the map\n",
    "$$\\Psi_{f}^{'}[g](x)=\\sigma[g](y_{k}+\\alpha\\nabla f(y)),\\qquad\\sigma[g](y)=\\mbox{argmin}\\{\\tfrac{1}{2}\\|\\bar{y}-y\\|^{2}+g(y)\\}$$\n",
    "\n",
    "so that, you guessed it\n",
    "\n",
    "1. $y^\\star$ is the unique fixed point of the map $y\\mapsto \\Psi_{f}'[g](y)$, i.e. $y^\\star = \\Psi_{f}'[g](y^\\star)$\n",
    "2. From any $y$, for small enough $\\alpha>0$ repeated application of the map will always converge to $y^\\star$\n",
    "\n",
    "See Boyd's [slides](https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf) for more details. A few notes are in order. First, though you can in principle stick in any $g$ you want, you cannot escape from having to compute $\\sigma_g$. This method hinges on when $\\sigma_g$ having something resembling a closed form solution. Second, this framework encompasses constrained optimization with a bit of syntactic sugar:\n",
    "$$\\underset{y\\in S}{\\mbox{argmin }}f(y)=\\underset{y}{\\mbox{argmin}}\\{f(y)+\\delta(y \\, | \\, S)\\},\\qquad\\delta(y)=\\begin{cases}\n",
    "0 & y\\in S\\\\\n",
    "\\infty & y\\notin S\n",
    "\\end{cases}.$$\n",
    "\n",
    "So the constrained problem can be \"simulated\" by using $g(y) = \\delta(y \\, | \\, S)$.\n",
    "\n",
    "For simple $g$'s, we can create a collection of its $\\sigma_g$'s. Here is a sampling\n",
    "$$\\begin{alignat*}{2}\n",
    " & g(y)=0 &  & \\sigma[g](y)=y\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{1} &  & \\sigma[g](y)_{i}=\\mbox{sign}(x_{i})(\\left|x_{i}\\right|-\\lambda)_{i}\\\\\n",
    " & g(y)=\\lambda\\|y\\|_{2} &  & \\sigma[g](y)=\\max\\{0,1-\\lambda/\\|y\\|_{2}\\}y\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+}) & \\qquad & \\sigma[g](y)_{i}=\\max\\{0,y_{i}\\}\\\\\n",
    " & g(y)=\\delta(y,\\mathbf{R}_{+})+\\lambda\\|y\\|_{1} & \\qquad & \\sigma[g](y)_{i}=\\max\\{0,y_{i}-\\lambda\\}\n",
    "\\end{alignat*}$$\n",
    "\n",
    "##### A new cornucopia of ARMs\n",
    "\n",
    "Now for each $g$, we have its corresponding Regression Machines, \n",
    "$$\\Psi[g](x)=\\underset{y}{\\mbox{argmin }}\\big\\{\\tfrac{1}{2}\\|Wy-x\\|^{2}+g(y)\\big\\}$$\n",
    "With corrosponding ARMs defined using the iterations of the map\n",
    "$$\\Psi[g]_{x}'(y):=\\sigma[g](y-\\alpha W^{T}(Wy-x))$$\n",
    "\n",
    "Take for example the Non-Negative Regression Machine, with $g(x)=\\delta(x,{\\mathbf R}_+)$. This has\n",
    "$$\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]=\\underset{y\\geq0}{\\mbox{argmin }}\\tfrac{1}{2}\\|Wy-x\\|^{2},\\quad\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]_{x}'(y):=\\max\\{0,y-\\alpha W^{T}(Wy-x)\\}$$\n",
    "With a 0-ARM of\n",
    "$$\\Psi[\\delta(\\cdot|\\mathbf{R}_{+})]_{0}(y)=\\max\\{0,W^{T}y\\},$$\n",
    "\n",
    "exactly a traditional layer with `ReLu` Activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on training the Network\n",
    "\n",
    "Finding the $W$'s for the network still remains a nagging problem. The usual set of tools apply, and given a large labeled dataset, the application of back-propagation on the $k$-ARM still works the way it should.\n",
    "\n",
    "##### Initialization via Matrix Factorization\n",
    "\n",
    "The paper proposes a heuristic for initializing $W$'s based on Dictionary Learning. For a single regression machine, assuming a dataset {$x_1,\\dots,x_n$} which possesses sparse linear structure, one could try to learn both the $y_i$'s and $W$'s simultaneously in the joint optimization problem\n",
    "$$\\underset{W,y_{1},\\dots,y_{n}}{\\mbox{argmin}}\\left\\{ \\sum_{i=1}^{n}\\left(\\tfrac{1}{2}\\|Wy_{i}-x_{i}\\|^{2}+\\lambda\\|y_{i}\\|_{1}\\right)\\right\\}.$$\n",
    "\n",
    "This is known as the [*Dictionary Learning Problem*](https://en.wikipedia.org/wiki/Sparse_dictionary_learning). Similarly, the Non-Negative ARM can be initialized via [Non-Negative Matrix Factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)\n",
    "$$\\underset{W\\geq0,y_i\\geq0}{\\mbox{argmin}}\\left\\{ \\sum_{i=1}^{n}\\left(\\tfrac{1}{2}\\|Wy_{i}-x_{i}\\|^{2}\\right)\\right\\} $$\n",
    "\n",
    "assuming the data is positive entrywise.\n",
    "\n",
    "These methods are all modifications of the mother of unsupervised learning, Principle Component Analysis. Like PCA, you are trying to find an explanation of the data as a small linear combinations of vectors. Unlike PCA, we have extra restrictions, like sparsity or non-negativity. The paper suggests finding a local solution to Dictionary Learning problem using $k$-PCA.\n",
    "\n",
    "Initialization for the a stacked ARM works as follows\n",
    "\n",
    "- Take the raw data, $x_i$, and solve the Dictionary Learning problem to yield $W$ and $y_i$'s.\n",
    "- Use $y_i$'s as the raw data for the next layer, and repeat.\n",
    "\n",
    "This technique eerily recalls our forgotten hero the [Restricted Boltzman Machine (RBM)](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine). Like the RBM, it too is a form of unsupervised pre-training. And like the RBM, it too learns weights greedily. Unlike the RBM, we don't need to deal with the uncomfortable transplantation of the weights from a RBM to a deep, directed network. The $k$-ARM unifies both initialization and inference, making our weight surgery less jarring. \n",
    "\n",
    "This rationalization may soothe those of us who crave order in our souls, but the most valuable proof is in the pudding. The stacked ARMs show exciting results on training data, and is a great first step in what I see as an exciting direction of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Oh, and here's a link to my [website](http://gabgoh.github.io).\n",
    "\n",
    "Picture in the title, as well as the sparse encoding picture, is taken from [Olshausen et al, Sparse Coding With An Overcomplete Basis Set: A Strategy Employed by V12?](http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       "/*div#maintoolbar, div#header {display: none !important;}\n",
       "*/\n",
       "\n",
       "#notebook {\n",
       "  padding-bottom: 20px\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "  font-family: Computer Modern\",Helvetica,Arial,sans-serif;\n",
       "  font-size:14px !important;\n",
       "  width:800px;\n",
       "  padding: 20px;\n",
       "  background-color: #FFF;\n",
       "  min-height: 10;\n",
       "  border: 0px solid;\n",
       "  box-shadow: 0px 0px 0px 0px !important;\n",
       "}\n",
       "\n",
       "\n",
       "div#notebook{\n",
       "font-size:16px;\n",
       "    line-height: 1.42857143;\n",
       "\n",
       "}\n",
       "\n",
       "body {\n",
       "  background-color: white !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display: {\n",
       "  margin: 20px\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: Computer Modern; src: url(\"http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf\");\n",
       "}\n",
       "\n",
       "\n",
       "div.cell{\n",
       "  width:700px;\n",
       "  margin-left:auto;\n",
       "  margin-right:auto;\n",
       "}\n",
       "\n",
       "div.output_subarea {\n",
       "    overflow-x: auto;\n",
       "    padding: 0.4em;\n",
       "    -webkit-box-flex: 1;\n",
       "    -moz-box-flex: 1;\n",
       "    box-flex: 1;\n",
       "    flex: 1;\n",
       "    max-width: calc(100%);\n",
       "}\n",
       "\n",
       ".output_wrapper {\n",
       "  padding-top : 10px;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "    border: 0px solid #cfcfcf;\n",
       "    border-radius: 2px;\n",
       "    background: rgba(255, 255, 255, 1);\n",
       "    line-height: 1.21429em;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       "h1 { font-family: \"Helvetica Neue\",Helvetica,Arial,sans-serif; }\n",
       "\n",
       ".prompt{ display:None; }\n",
       "\n",
       ".output_png img {\n",
       "    display: block !important;\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".output_svg > div{\n",
       "margin-left:auto !important;\n",
       "margin-right:auto !important;\n",
       "}\n",
       "\n",
       ".ui-wrapper {\n",
       "  margin-left:auto !important;\n",
       "  margin-right:auto !important\n",
       "}\n",
       "\n",
       "hr {\n",
       "    display: block;\n",
       "    height: 1px;\n",
       "    border: 0;\n",
       "    border-top: 1px solid #000;\n",
       "    margin: 1em 0;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".err{\n",
       "  border : 0px !important;\n",
       "}\n",
       "\n",
       ".MathJax_Display{\n",
       "  padding-top: 12px;\n",
       "}\n",
       "\n",
       "\n",
       ".hl-ipython3, .hl-ipython2 {\n",
       "  display: none\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       "  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       "  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       "  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       "\n",
       "  ga('create', 'UA-65931696-1', 'auto');\n",
       "  ga('send', 'pageview');\n",
       "\n",
       "</script>\n",
       "\n",
       "<div id=\"disqus_thread\"></div>\n",
       "<script>\n",
       "\n",
       "/**\n",
       " *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.\n",
       " *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables */\n",
       "/*\n",
       "var disqus_config = function () {\n",
       "    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable\n",
       "    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable\n",
       "};\n",
       "*/\n",
       "(function() { // DON'T EDIT BELOW THIS LINE\n",
       "    var d = document, s = d.createElement('script');\n",
       "    s.src = '//gabes-home-on-the-web.disqus.com/embed.js';\n",
       "    s.setAttribute('data-timestamp', +new Date());\n",
       "    (d.head || d.body).appendChild(s);\n",
       "})();\n",
       "</script>\n",
       "<noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(open(\"style.css\",\"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
